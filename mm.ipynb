{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g 512,512\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels=32,\n",
    "                #  internal_ratio=4,\n",
    "                 kernel_size=3,\n",
    "                 padding=0,\n",
    "                 dropout_prob=0.,\n",
    "                 stride = 2,\n",
    "                 bias=False,\n",
    "                 relu=True,\n",
    "                 out_channels_div= [11,9,9] \n",
    "            ):\n",
    "        super(Downsample,self).__init__()\n",
    "\n",
    "\n",
    "        if relu:\n",
    "            activation = nn.ReLU()\n",
    "        else:\n",
    "            activation = nn.PReLU()\n",
    "\n",
    "        self.conv2d_d1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels_div[0],kernel_size=kernel_size, stride=stride, padding=padding, bias=bias, dilation=1)\n",
    "        self.conv2d_d2 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels_div[1],kernel_size=kernel_size, stride=stride, padding=1, bias=bias, dilation=2)\n",
    "        self.conv2d_d5 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels_div[2],kernel_size=kernel_size, stride=stride, padding=4, bias=bias, dilation=5)\n",
    "        self.ext_branch = nn.MaxPool2d(kernel_size, stride=2, padding=padding)\n",
    "        # self.half = nn.MaxPool2d(2, stride=2)\n",
    "        self.activation = activation\n",
    "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, input):\n",
    "        print('input:', input.shape)\n",
    "        main1_d1= self.conv2d_d1(input)\n",
    "        print('dialation 1:',main1_d1.shape)\n",
    "        main2_d2 = self.conv2d_d2(input)\n",
    "        print('dialation 2', main2_d2.shape)\n",
    "        main3_d5 = self.conv2d_d5(input)\n",
    "        print('dialation 5:',main3_d5.shape)\n",
    "        # ext1=self.half(input)\n",
    "        ext1 = self.ext_branch(input)\n",
    "        print(ext1.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        out = torch.cat((main1_d1,main2_d2,main3_d5,ext1), dim=1)\n",
    "        print('out after concate', out.shape)\n",
    "        out = self.batch_norm(out)\n",
    "        out = self.activation(out)\n",
    "        # print('final', out)\n",
    "\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Downsample(in_channels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input: torch.Size([1, 3, 360, 640])\n",
      "dialation 1: torch.Size([1, 11, 179, 319])\n",
      "dialation 2 torch.Size([1, 9, 179, 319])\n",
      "dialation 5: torch.Size([1, 9, 179, 319])\n",
      "torch.Size([1, 3, 179, 319])\n",
      "out after concate torch.Size([1, 32, 179, 319])\n",
      "C:\\Users\\abdul\\.conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1,3, 360, 640)\n",
    "xx = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downsample(\n  (conv2d_d1): Conv2d(3, 11, kernel_size=(3, 3), stride=(2, 2), bias=False)\n  (conv2d_d2): Conv2d(3, 9, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(2, 2), bias=False)\n  (conv2d_d5): Conv2d(3, 9, kernel_size=(3, 3), stride=(2, 2), padding=(4, 4), dilation=(5, 5), bias=False)\n  (ext_branch): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (activation): ReLU()\n  (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g 512,512\n",
    "\n",
    "class Regular(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels=32,\n",
    "                 out_channels=32,\n",
    "                #  internal_ratio=4,\n",
    "                 kernel_size=3,\n",
    "                 padding=0,\n",
    "                 dropout_prob=0.,\n",
    "                 stride = 1,\n",
    "                 bias=False,\n",
    "                 relu=True, \n",
    "                 out_channels_div = [11,11,10]\n",
    "                 \n",
    "                 ):\n",
    "        super(Regular,self).__init__()\n",
    "\n",
    "\n",
    "        if relu:\n",
    "            activation = nn.ReLU()\n",
    "        else:\n",
    "            activation = nn.PReLU()\n",
    "\n",
    "        self.conv2d_d1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels_div[0],kernel_size=kernel_size, stride=stride, padding=1, bias=bias, dilation=1)\n",
    "        self.conv2d_d2 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels_div[1],kernel_size=kernel_size, stride=stride, padding=2, bias=bias, dilation=2)\n",
    "        self.conv2d_d5 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels_div[2],kernel_size=kernel_size, stride=stride, padding=5, bias=bias, dilation=5)\n",
    "        # self.half = nn.MaxPool2d(2, stride=2)\n",
    "        self.activation = activation\n",
    "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # print('input:', input.shape)\n",
    "        main1_d1= self.conv2d_d1(input)\n",
    "        print('dialation 1:',main1_d1.shape)\n",
    "        main2_d2 = self.conv2d_d2(input)\n",
    "        print('dialation 2', main2_d2.shape)\n",
    "        main3_d5 = self.conv2d_d5(input)\n",
    "        print('dialation 5:',main3_d5.shape)\n",
    "        # ext1=self.half(input)\n",
    "        \n",
    "        \n",
    "        \n",
    "        out = torch.cat((main1_d1,main2_d2,main3_d5), dim=1)\n",
    "        print('out after concate', out.shape)\n",
    "        out = self.batch_norm(out)\n",
    "        out = self.activation(out)\n",
    "        # print('final', out)\n",
    "\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Regular(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dialation 1: torch.Size([1, 11, 360, 640])\ndialation 2 torch.Size([1, 11, 360, 640])\ndialation 5: torch.Size([1, 10, 360, 640])\nout after concate torch.Size([1, 32, 360, 640])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[0.0000, 0.4039, 0.0000,  ..., 0.0000, 0.8166, 0.0000],\n",
       "          [0.6406, 0.0000, 0.5942,  ..., 0.0038, 0.9530, 1.5304],\n",
       "          [0.5986, 0.0000, 0.1506,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.8491, 0.0000, 0.0000,  ..., 0.4574, 0.0000, 0.9222],\n",
       "          [0.0000, 0.7526, 0.0000,  ..., 0.0000, 0.0000, 0.6954],\n",
       "          [0.0000, 0.0000, 0.3739,  ..., 0.0000, 0.3922, 0.0000]],\n",
       "\n",
       "         [[0.0000, 1.0010, 0.1104,  ..., 0.0000, 0.6418, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 1.3113, 3.4148, 0.8746],\n",
       "          [0.8499, 0.0000, 0.7296,  ..., 0.0000, 0.4518, 0.0000],\n",
       "          ...,\n",
       "          [1.1909, 0.0000, 0.0000,  ..., 0.6962, 0.0000, 0.9692],\n",
       "          [0.6480, 1.4951, 0.0000,  ..., 0.0000, 0.9164, 0.8673],\n",
       "          [0.0000, 0.0000, 1.1174,  ..., 0.0000, 0.0000, 0.8118]],\n",
       "\n",
       "         [[0.0000, 0.4219, 1.0575,  ..., 0.1947, 0.4190, 0.0899],\n",
       "          [0.0000, 0.5272, 0.9461,  ..., 0.0000, 0.3880, 0.0000],\n",
       "          [0.3897, 1.1210, 2.0873,  ..., 0.0000, 0.6073, 1.2157],\n",
       "          ...,\n",
       "          [0.0000, 0.8717, 0.0000,  ..., 0.5705, 0.1465, 0.2649],\n",
       "          [0.7026, 0.0000, 0.0000,  ..., 1.8655, 0.0000, 0.3403],\n",
       "          [0.0000, 1.4189, 0.0000,  ..., 0.2553, 1.0470, 0.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.4616, 0.0000, 0.0000,  ..., 0.0000, 0.3052, 0.4374],\n",
       "          [0.9687, 0.0569, 0.2440,  ..., 0.0000, 0.6935, 0.0000],\n",
       "          [0.4221, 1.2168, 0.0000,  ..., 1.3068, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.2569, 0.1570,  ..., 0.0000, 0.0488, 0.0000],\n",
       "          [0.3691, 0.4063, 1.3802,  ..., 0.0512, 0.0000, 0.0000],\n",
       "          [1.0842, 0.2905, 0.6583,  ..., 0.4840, 0.8201, 0.0000]],\n",
       "\n",
       "         [[0.9852, 0.0000, 0.0730,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.5387, 0.5258,  ..., 0.0000, 0.4224, 0.2219],\n",
       "          [0.0000, 0.8419, 0.0826,  ..., 0.1833, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 2.3005,  ..., 0.0000, 0.0192, 0.0955],\n",
       "          [0.0109, 0.0000, 0.2673,  ..., 0.6364, 0.0000, 0.0000],\n",
       "          [1.4243, 0.6173, 0.0000,  ..., 0.5130, 0.1143, 0.3186]],\n",
       "\n",
       "         [[0.0000, 0.9815, 0.0000,  ..., 0.0000, 0.9588, 0.7381],\n",
       "          [0.0000, 0.5785, 0.0000,  ..., 0.5445, 0.0000, 0.0000],\n",
       "          [1.2291, 0.0000, 0.0000,  ..., 0.3911, 0.6002, 0.6416],\n",
       "          ...,\n",
       "          [0.4019, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0895],\n",
       "          [0.2720, 0.0000, 0.0000,  ..., 0.9423, 0.0457, 0.3266],\n",
       "          [0.0000, 0.5153, 0.1099,  ..., 0.1867, 0.5451, 0.4824]]]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "model2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Regular(\n  (conv2d_d1): Conv2d(3, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (conv2d_d2): Conv2d(3, 11, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n  (conv2d_d5): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(5, 5), dilation=(5, 5), bias=False)\n  (activation): ReLU()\n  (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)\n"
     ]
    }
   ],
   "source": [
    "print(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, classes, encoder_relu=False, decoder_relu=True):\n",
    "        super().__init__()\n",
    "        self.downsample_1 = Downsample(in_channels=3,out_channels=32, kernel_size=3, relu=True, out_channels_div=[11,9,9])\n",
    "        self.regular_1 = Regular(in_channels=32, out_channels=32, kernel_size=3, out_channels_div=[11,11,10])\n",
    "        self.downsample_2 = Downsample(in_channels=32, out_channels=64, kernel_size=3,relu=True, out_channels_div=[11,11,10])\n",
    "        self.regular_2 = Regular(in_channels=64, out_channels=64,out_channels_div=[22,21,21])\n",
    "        self.downsample_3 = Downsample(in_channels=64, out_channels=128, out_channels_div=[22,21,21])\n",
    "        self.regular_3 = Regular(in_channels=128, out_channels=128, out_channels_div=[43,43,42])\n",
    "        self.regular_4 = Regular(in_channels=128, out_channels=128, out_channels_div=[43,43,42] )\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.downsample_1(input)\n",
    "        x = self.regular_1(x)\n",
    "        x = self.downsample_2(x)\n",
    "        x = self.regular_2(x)\n",
    "        x = self.downsample_3(x)\n",
    "        x= self.regular_3(x)\n",
    "        x= self.regular_4(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input: torch.Size([1, 3, 360, 640])\ndialation 1: torch.Size([1, 11, 179, 319])\ndialation 2 torch.Size([1, 9, 179, 319])\ndialation 5: torch.Size([1, 9, 179, 319])\ntorch.Size([1, 3, 179, 319])\nout after concate torch.Size([1, 32, 179, 319])\ndialation 1: torch.Size([1, 11, 179, 319])\ndialation 2 torch.Size([1, 11, 179, 319])\ndialation 5: torch.Size([1, 10, 179, 319])\nout after concate torch.Size([1, 32, 179, 319])\ninput: torch.Size([1, 32, 179, 319])\ndialation 1: torch.Size([1, 11, 89, 159])\ndialation 2 torch.Size([1, 11, 89, 159])\ndialation 5: torch.Size([1, 10, 89, 159])\ntorch.Size([1, 32, 89, 159])\nout after concate torch.Size([1, 64, 89, 159])\ndialation 1: torch.Size([1, 22, 89, 159])\ndialation 2 torch.Size([1, 21, 89, 159])\ndialation 5: torch.Size([1, 21, 89, 159])\nout after concate torch.Size([1, 64, 89, 159])\ninput: torch.Size([1, 64, 89, 159])\ndialation 1: torch.Size([1, 22, 44, 79])\ndialation 2 torch.Size([1, 21, 44, 79])\ndialation 5: torch.Size([1, 21, 44, 79])\ntorch.Size([1, 64, 44, 79])\nout after concate torch.Size([1, 128, 44, 79])\ndialation 1: torch.Size([1, 43, 44, 79])\ndialation 2 torch.Size([1, 43, 44, 79])\ndialation 5: torch.Size([1, 42, 44, 79])\nout after concate torch.Size([1, 128, 44, 79])\ndialation 1: torch.Size([1, 43, 44, 79])\ndialation 2 torch.Size([1, 43, 44, 79])\ndialation 5: torch.Size([1, 42, 44, 79])\nout after concate torch.Size([1, 128, 44, 79])\ntensor([[[[1.2244e-01, 0.0000e+00, 8.7823e-01,  ..., 6.3820e-01,\n           4.2192e-01, 0.0000e+00],\n          [7.4153e-01, 6.7481e-01, 1.1900e+00,  ..., 4.0275e-01,\n           4.3811e-04, 3.9003e-01],\n          [1.1324e+00, 8.5929e-01, 2.8467e-02,  ..., 8.3474e-01,\n           0.0000e+00, 0.0000e+00],\n          ...,\n          [1.8066e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n           1.0121e+00, 0.0000e+00],\n          [8.5671e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n           0.0000e+00, 0.0000e+00],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n           4.5669e-03, 0.0000e+00]],\n\n         [[1.1713e+00, 0.0000e+00, 5.0219e-02,  ..., 4.7275e-01,\n           6.3866e-01, 4.4836e-01],\n          [1.2557e+00, 3.2509e-01, 2.7412e-01,  ..., 0.0000e+00,\n           0.0000e+00, 1.8904e-01],\n          [1.5675e+00, 1.2739e-01, 5.6832e-01,  ..., 0.0000e+00,\n           0.0000e+00, 0.0000e+00],\n          ...,\n          [8.7591e-01, 5.9836e-01, 0.0000e+00,  ..., 4.3102e-01,\n           1.2164e+00, 5.1649e-01],\n          [8.3443e-01, 1.1868e+00, 0.0000e+00,  ..., 1.5591e-01,\n           1.1577e+00, 4.8002e-02],\n          [5.9063e-01, 0.0000e+00, 0.0000e+00,  ..., 5.6929e-01,\n           0.0000e+00, 0.0000e+00]],\n\n         [[0.0000e+00, 5.2109e-01, 7.3852e-01,  ..., 1.4078e-01,\n           0.0000e+00, 0.0000e+00],\n          [0.0000e+00, 0.0000e+00, 9.8822e-01,  ..., 0.0000e+00,\n           0.0000e+00, 0.0000e+00],\n          [0.0000e+00, 0.0000e+00, 8.9403e-02,  ..., 0.0000e+00,\n           0.0000e+00, 0.0000e+00],\n          ...,\n          [0.0000e+00, 1.0809e+00, 5.7741e-01,  ..., 6.6972e-01,\n           1.4415e+00, 0.0000e+00],\n          [0.0000e+00, 3.7038e-01, 1.1028e+00,  ..., 9.6643e-01,\n           4.1831e-01, 9.5525e-01],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0190e+00,\n           8.2855e-03, 6.1561e-01]],\n\n         ...,\n\n         [[2.0983e-01, 6.9176e-01, 1.1202e+00,  ..., 0.0000e+00,\n           5.4579e-02, 0.0000e+00],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n           0.0000e+00, 0.0000e+00],\n          [6.2668e-02, 0.0000e+00, 6.7538e-01,  ..., 0.0000e+00,\n           2.9495e-01, 0.0000e+00],\n          ...,\n          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n           0.0000e+00, 0.0000e+00],\n          [1.1486e+00, 7.8925e-02, 9.9898e-02,  ..., 0.0000e+00,\n           1.9724e-01, 0.0000e+00],\n          [1.6368e-01, 2.3034e-01, 0.0000e+00,  ..., 0.0000e+00,\n           0.0000e+00, 0.0000e+00]],\n\n         [[2.7795e-01, 9.0071e-02, 4.3924e-02,  ..., 1.4760e+00,\n           3.3872e-01, 0.0000e+00],\n          [1.2079e-01, 1.1291e-02, 4.9379e-01,  ..., 8.4629e-01,\n           8.5465e-01, 3.4666e-01],\n          [2.8960e-01, 6.3747e-01, 0.0000e+00,  ..., 4.8584e-01,\n           0.0000e+00, 5.2252e-01],\n          ...,\n          [0.0000e+00, 0.0000e+00, 3.6504e-01,  ..., 0.0000e+00,\n           3.5869e-01, 0.0000e+00],\n          [0.0000e+00, 3.4681e-01, 3.1822e-01,  ..., 0.0000e+00,\n           0.0000e+00, 0.0000e+00],\n          [0.0000e+00, 2.2081e-01, 9.0095e-01,  ..., 0.0000e+00,\n           0.0000e+00, 0.0000e+00]],\n\n         [[9.4116e-01, 1.2075e+00, 5.7602e-01,  ..., 5.2846e-01,\n           2.7648e-01, 4.3518e-01],\n          [8.3535e-01, 1.7411e+00, 1.1389e+00,  ..., 9.2786e-01,\n           4.3128e-02, 0.0000e+00],\n          [4.9347e-01, 7.8757e-01, 3.1526e-01,  ..., 4.8459e-01,\n           7.4865e-01, 1.4851e-01],\n          ...,\n          [0.0000e+00, 2.4761e-01, 2.0276e-01,  ..., 0.0000e+00,\n           0.0000e+00, 2.5077e-01],\n          [0.0000e+00, 5.3334e-01, 0.0000e+00,  ..., 0.0000e+00,\n           0.0000e+00, 2.6982e-01],\n          [0.0000e+00, 2.0901e-01, 1.2173e+00,  ..., 7.7585e-01,\n           2.5666e-02, 1.0253e+00]]]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = encoder(x)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Encoder(\n  (downsample_1): Downsample(\n    (conv2d_d1): Conv2d(3, 11, kernel_size=(3, 3), stride=(2, 2), bias=False)\n    (conv2d_d2): Conv2d(3, 9, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(2, 2), bias=False)\n    (conv2d_d5): Conv2d(3, 9, kernel_size=(3, 3), stride=(2, 2), padding=(4, 4), dilation=(5, 5), bias=False)\n    (ext_branch): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (activation): ReLU()\n    (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (regular_1): Regular(\n    (conv2d_d1): Conv2d(32, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (conv2d_d2): Conv2d(32, 11, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n    (conv2d_d5): Conv2d(32, 10, kernel_size=(3, 3), stride=(1, 1), padding=(5, 5), dilation=(5, 5), bias=False)\n    (activation): ReLU()\n    (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (downsample_2): Downsample(\n    (conv2d_d1): Conv2d(32, 11, kernel_size=(3, 3), stride=(2, 2), bias=False)\n    (conv2d_d2): Conv2d(32, 11, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(2, 2), bias=False)\n    (conv2d_d5): Conv2d(32, 10, kernel_size=(3, 3), stride=(2, 2), padding=(4, 4), dilation=(5, 5), bias=False)\n    (ext_branch): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (activation): ReLU()\n    (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (regular_2): Regular(\n    (conv2d_d1): Conv2d(64, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (conv2d_d2): Conv2d(64, 21, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n    (conv2d_d5): Conv2d(64, 21, kernel_size=(3, 3), stride=(1, 1), padding=(5, 5), dilation=(5, 5), bias=False)\n    (activation): ReLU()\n    (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (downsample_3): Downsample(\n    (conv2d_d1): Conv2d(64, 22, kernel_size=(3, 3), stride=(2, 2), bias=False)\n    (conv2d_d2): Conv2d(64, 21, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(2, 2), bias=False)\n    (conv2d_d5): Conv2d(64, 21, kernel_size=(3, 3), stride=(2, 2), padding=(4, 4), dilation=(5, 5), bias=False)\n    (ext_branch): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (activation): ReLU()\n    (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (regular_3): Regular(\n    (conv2d_d1): Conv2d(128, 43, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (conv2d_d2): Conv2d(128, 43, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n    (conv2d_d5): Conv2d(128, 42, kernel_size=(3, 3), stride=(1, 1), padding=(5, 5), dilation=(5, 5), bias=False)\n    (activation): ReLU()\n    (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (regular_4): Regular(\n    (conv2d_d1): Conv2d(128, 43, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (conv2d_d2): Conv2d(128, 43, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n    (conv2d_d5): Conv2d(128, 42, kernel_size=(3, 3), stride=(1, 1), padding=(5, 5), dilation=(5, 5), bias=False)\n    (activation): ReLU()\n    (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size = 3, relu = True):\n",
    "        super(Upsample,self).__init__()\n",
    "\n",
    "        if relu:\n",
    "            activation = nn.ReLU\n",
    "        else:\n",
    "            activation = nn.PReLU\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        self.conv = nn.ConvTranspose2d(in_channels= in_channels, \n",
    "                                        out_channels=out_channels, stride=2,\n",
    "                                        kernel_size=kernel_size, \n",
    "                                        padding=0, output_padding=0,\n",
    "                                        bias=True\n",
    "                                        )\n",
    "        # self.bn = nn.BatchNorm2d(noutput, eps=1e-3)     # ERFNet batchnorm with eps = e-5\n",
    "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
    "        # self.regular_de_1=Regular(in_channels=64,out_channels=64 ,out_channels_div=[22,21,21])\n",
    "        self.activation = activation\n",
    "    def forward(self, input):\n",
    "        print('Input Shape: ', input.shape)\n",
    "        output = self.conv(input)\n",
    "        output = self.batch_norm(output)\n",
    "        print('Output Shape: ', output.shape)\n",
    "        output = self.activation(output)\n",
    "        \n",
    "        # output = self.regular_de_1(output)\n",
    "        # output = nn.ReLU(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Upsample(in_channels=128, out_channels=64) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Upsample(\n",
       "  (conv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "t= torch.rand(1,128,44,79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input Shape:  torch.Size([1, 128, 44, 79])\nOutput Shape:  torch.Size([1, 64, 89, 159])\n"
     ]
    }
   ],
   "source": [
    "yy =test(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g 512,512\n",
    "\n",
    "class Regular_decode(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels=32,\n",
    "                 out_channels=32,\n",
    "                #  internal_ratio=4,\n",
    "                 kernel_size=3,\n",
    "                 padding=0,\n",
    "                 dropout_prob=0.,\n",
    "                 stride = 1,\n",
    "                 bias=False,\n",
    "                 relu=True, \n",
    "                 out_channels_div = [11,11,10]\n",
    "                 \n",
    "                 ):\n",
    "        super(Regular_decode,self).__init__()\n",
    "\n",
    "\n",
    "        if relu:\n",
    "            activation = nn.ReLU()\n",
    "        else:\n",
    "            activation = nn.PReLU()\n",
    "\n",
    "        self.conv2d_d1 = nn.Conv2d(in_channels=in_channels, \n",
    "                                    out_channels=out_channels_div[0],\n",
    "                                    kernel_size=kernel_size, \n",
    "                                    stride=stride, \n",
    "                                    padding=1, \n",
    "                                    bias=bias, \n",
    "                                    dilation=1\n",
    "                                    )\n",
    "\n",
    "        self.conv2d_d2 = nn.Conv2d(in_channels=in_channels, \n",
    "                                    out_channels=out_channels_div[1],\n",
    "                                    kernel_size=kernel_size, \n",
    "                                    stride=stride, \n",
    "                                    padding=2, \n",
    "                                    bias=bias, \n",
    "                                    dilation=2\n",
    "                                    )\n",
    "\n",
    "        self.conv2d_d5 = nn.Conv2d(in_channels=in_channels, \n",
    "                                    out_channels=out_channels_div[2],\n",
    "                                    kernel_size=kernel_size, \n",
    "                                    stride=stride, padding=5, \n",
    "                                    bias=bias, \n",
    "                                    dilation=5\n",
    "                                    )\n",
    "\n",
    "        # self.half = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # print('input:', input.shape)\n",
    "        main1_d1= self.conv2d_d1(input)\n",
    "        print('dialation 1:',main1_d1.shape)\n",
    "        main2_d2 = self.conv2d_d2(input)\n",
    "        print('dialation 2', main2_d2.shape)\n",
    "        main3_d5 = self.conv2d_d5(input)\n",
    "        print('dialation 5:',main3_d5.shape)\n",
    "        # ext1=self.half(input)\n",
    "        \n",
    "        \n",
    "        \n",
    "        out = torch.cat((main1_d1,main2_d2,main3_d5), dim=1)\n",
    "        print('out after concate', out.shape)\n",
    "        out = self.batch_norm(out)\n",
    "        out = self.activation(out)\n",
    "        # print('final', out)\n",
    "\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = Regular_decode(in_channels=128, out_channels=64 ,out_channels_div=[22,21,21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "t= torch.rand(1,128,89,159)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dialation 1: torch.Size([1, 22, 89, 159])\ndialation 2 torch.Size([1, 21, 89, 159])\ndialation 5: torch.Size([1, 21, 89, 159])\nout after concate torch.Size([1, 64, 89, 159])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 1.9491, 0.0000,  ..., 0.9291, 0.7268, 0.1285],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0153, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0770, 1.1516,  ..., 0.0000, 0.4107, 0.0000],\n",
       "          [0.0000, 0.0237, 0.0000,  ..., 0.0000, 0.9543, 0.0000],\n",
       "          [0.2365, 0.0000, 0.0000,  ..., 0.0000, 1.5381, 0.4400]],\n",
       "\n",
       "         [[0.0000, 0.1946, 0.3209,  ..., 1.3862, 0.3922, 0.6012],\n",
       "          [0.0000, 1.1482, 0.0000,  ..., 1.1905, 0.0000, 0.9589],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4686, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.6855],\n",
       "          [0.0000, 1.7284, 0.8159,  ..., 1.1612, 1.0018, 0.1172],\n",
       "          [0.0000, 0.4396, 0.0000,  ..., 0.8046, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.4905],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.5633, 0.2951, 0.4702],\n",
       "          [0.0000, 0.3210, 0.0000,  ..., 0.5898, 0.1328, 1.7776],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.1881, 1.0122, 0.4929],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6409, 1.9980],\n",
       "          [0.0000, 1.6195, 1.9203,  ..., 1.4829, 1.1227, 0.1699]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.6968, 1.4866, 2.0302,  ..., 2.4876, 2.1319, 2.1363],\n",
       "          [1.3893, 1.5137, 1.2378,  ..., 2.8077, 2.5782, 2.6602],\n",
       "          [2.6108, 2.0648, 1.7651,  ..., 2.5781, 2.6258, 2.8106],\n",
       "          ...,\n",
       "          [0.2213, 0.3092, 0.6423,  ..., 1.6803, 0.9603, 1.0978],\n",
       "          [0.0000, 1.8349, 1.4272,  ..., 1.8028, 1.3656, 1.4954],\n",
       "          [0.2050, 0.9404, 1.3764,  ..., 0.4923, 1.8830, 1.5334]],\n",
       "\n",
       "         [[0.0000, 0.6689, 0.0000,  ..., 2.0360, 1.0960, 0.9369],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 1.4094, 0.0000, 0.6691],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.6935, 1.1948, 0.9337],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 1.0305,  ..., 0.0192, 0.0000, 0.0000],\n",
       "          [0.0000, 0.4972, 0.0000,  ..., 0.7385, 2.2196, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.2853, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4163, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "test_1(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.upsample_1 = Upsample(in_channels=128, out_channels=64)\n",
    "        self.regular_de_1 = Regular_decode(in_channels=64, out_channels=64, out_channels_div=[22,21,21])\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.upsample_1(input)\n",
    "        x = self.regular_de_1(x)\n",
    "        \n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Decoder(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = torch.rand(1,128,44,79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 44, 79])"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "rand.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (upsample_1): Upsample(\n",
       "    (conv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (regular_de_1): Regular_decode(\n",
       "    (conv2d_d1): Conv2d(64, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (conv2d_d2): Conv2d(64, 21, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "    (conv2d_d5): Conv2d(64, 21, kernel_size=(3, 3), stride=(1, 1), padding=(5, 5), dilation=(5, 5), bias=False)\n",
       "    (activation): ReLU()\n",
       "    (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 302
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input Shape:  torch.Size([1, 128, 44, 79])\nOutput Shape:  torch.Size([1, 64, 89, 159])\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (ReLU, Parameter, NoneType, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!ReLU!, !Parameter!, !NoneType!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!ReLU!, !Parameter!, !NoneType!, !tuple!, !tuple!, !tuple!, int)\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-1b401aa5315e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mouput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-b853a80623b9>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupsample_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregular_de_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-b4f3ded9222d>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;31m# print('input:', input.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mmain1_d1\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d_d1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dialation 1:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmain1_d1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mmain2_d2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d_d2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m--> 440\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (ReLU, Parameter, NoneType, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!ReLU!, !Parameter!, !NoneType!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!ReLU!, !Parameter!, !NoneType!, !tuple!, !tuple!, !tuple!, int)\n"
     ]
    }
   ],
   "source": [
    "ouput = model(rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e02bb32617b42604c8262daf80a9a642b645a80e814d98eac5f235524f385bd"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('torch': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}